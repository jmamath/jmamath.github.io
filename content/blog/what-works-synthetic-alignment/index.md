---
title: "What Works in Synthetic Alignment: Evidence and Scorecard"
date: 2025-11-13
summary: "Examining empirical evidence: which RLHF limitations have been solved, which remain partially addressed, and six key insights that shape the field."
tags: ["AI", "RLAIF", "Machine Learning", "Alignment", "Research", "Evaluation"]
authors: ["admin"]
---

> **Part 3 of 4** in the series "From Human Feedback to Synthetic Alignment"
>
> **[← Part 2: Architecture](/blog/synthetic-alignment-architecture/)** | **[Series Overview](/blog/synthetic-alignment-overview/)** | **[Part 4: Research Frontiers →](/blog/synthetic-alignment-future/)**
>
> **[Part 1](/blog/rlhf-limitations/)** established RLHF's fundamental limitations. **[Part 2](/blog/synthetic-alignment-architecture/)** mapped the design space of synthetic alignment methods. This article examines the empirical evidence: which RLHF limitations have been solved, which remain, and what six key insights shape the field. **[Part 4](/blog/synthetic-alignment-future/)** explores critical open research questions.

---

## What the Evidence Tells Us

[Part 2](/blog/synthetic-alignment-architecture/)'s framework of two paradigms and eight design factors provides analytical clarity, but the real value emerges when we examine what the empirical evidence reveals. These factors aren't just taxonomic categories, they're levers that researchers have pulled in different combinations, generating a body of evidence about what actually works. By analyzing papers through this lens, particularly those with shared evaluation frameworks, we can draw concrete conclusions about which choices matter most.

The importance of Factor 8 (Evaluation Methodology) becomes clear here: it determines which papers we can meaningfully compare. The cluster of Yuan et al. (2025), Wu et al. (2024), and Dong et al. (2024) sharing AlpacaEval 2.0 enables us to isolate the impact of specific choices—Wu et al.'s explicit judge training versus Yuan et al.'s emergent judging, Dong et al.'s prompt diversity emphasis versus other factors. Similarly, Cheng et al.'s (2025) explicit comparison against Yuan et al. and Wu et al. on IFEval provides clean attribution of their superior performance to tree-search refinement specifically. Without shared evaluation frameworks, we're left making educated guesses rather than drawing evidence-based conclusions.

### Six Empirical Insights That Shape the Field

Certain patterns emerge with sufficient consistency and supporting evidence to constitute genuine insights about synthetic alignment. These aren't speculative principles—they're conclusions grounded in comparable empirical work.

**1. On-Policy Training Dominates Offline Approaches**

The evidence here is particularly strong. Guo et al. (2024) demonstrate that online DPO wins human preference comparisons 58% of the time in a rigorous 4-way evaluation against offline DPO, SFT baselines, RLHF, and RLAIF on TL;DR summarization. More importantly, online training exhibits significantly more stable training dynamics than offline DPO. This isn't a marginal advantage—it addresses one of the major practical concerns that has historically plagued preference optimization. The mechanism is clear: maintaining distribution alignment between the policy and training data prevents the reward model staleness and distribution mismatch that degrade offline methods as training progresses.

**2. Explicit Judge Training Outperforms Emergent Capabilities**

Wu et al. (2024) surpass Yuan et al. (2025) on AlpacaEval 2.0 despite using the same fundamental DPO architecture. The authors attribute the improvement to explicitly training the judge in each iteration rather than hoping that generation capability naturally confers evaluation ability. This demonstrates a broader principle: co-evolving the judge with the policy through explicit training prevents the judge from becoming stale or misaligned as the policy's distribution shifts. Emergent capabilities aren't sufficient—dedicated judge training matters.

**3. Data Quality is Multifaceted, Not Unidimensional**

Several papers reveal that "higher quality data" means different things in different contexts. Shi et al. (2024) show that source authenticity matters: generating unsafe prompts from real-world harmful text corpora (hate speech, self-harm content, illegal activities, sexual content) outperforms training on synthetic Anthropic HH data for harmlessness while preserving helpfulness. Authentic examples create more effective training signals than purely synthetic generation.

Dong et al. (2024) identify prompt diversity as the dominant factor: "most of the improvement comes from the prompt diversity" generated by their self-prompt generator, suggesting that coverage across instruction types competes with individual response quality for importance.

Cheng et al. (2025) demonstrate the value of computational investment during data collection: their tree-search refinement systematically explores better responses during generation, contributing to their ability to surpass GPT-4-Turbo on IFEval after just 3 iterations. The authors observe that "more test time compute is spent appropriately as they iterate"—compute during data collection, not just inference, improves final model quality.

Finally, Cui et al. (2024) show that reward model training data quality matters: fine-tuning UltraRM on high-quality synthetic feedback (multi-dimensional scores plus critiques) in addition to open-source data yields better performance than open-source data alone.

**4. Self-Improvement Hits Diminishing Returns After 3-4 Iterations**

A consistent pattern emerges across iterative methods: performance gains diminish after 3-4 iterations. Wu et al. (2024) observe judge scores clustering around 5 after iteration 4, suggesting response homogeneity limits further improvement. The authors propose more flexible scoring systems as a potential solution, but the pattern appears across multiple approaches with different architectures—suggesting a fundamental challenge rather than a method-specific bug. The implication is sobering: current synthetic alignment methods may face an inherent ceiling on how far they can self-improve without architectural innovations.

**5. Careful Alignment Preserves General Capabilities**

The fear that alignment degrades general capabilities finds little support in the evidence. Dong et al.'s (2024) comprehensive regression testing across 12 benchmarks shows that alignment can improve target capabilities without degrading others. Llama-8B shows improvement or neutrality across all benchmarks. Cheng et al. (2025) confirm no average regression on GSM8k, TriviaQA, MMLU, and HumanEval. The apparent trade-off between alignment and general capability often discussed in the literature doesn't materialize in practice when alignment is done carefully. Specialized capabilities can emerge without sacrificing general ones.

**6. Controlled Comparisons Enable Clean Attribution**

The most valuable contributions isolate specific design choices through controlled comparisons. Cheng et al.'s (2025) explicit comparison against Yuan et al. (2025) and Wu et al. (2024) using identical data on IFEval enables clean attribution of their superior performance (surpassing GPT-4-Turbo) specifically to tree-search refinement. Without such controlled comparisons, improvements could stem from algorithm design, data quality, hyperparameter choices, or any number of confounding factors. The field needs more such comparative work to move beyond anecdata toward genuine scientific understanding of which choices matter.

---

**A Final Note on Paradigm Boundaries**

Zweiger et al.'s (2025) work on self-adapting language models represents a different paradigm entirely, meta-learning and test-time training rather than traditional preference-based alignment. Its inclusion in this analysis highlights that self-improvement mechanisms have broader applicability than preference optimization alone. The boundaries of synthetic alignment continue to expand as researchers explore new architectural possibilities beyond the RL-based and direct optimization paradigms that currently dominate.

---

# From RLHF's Limitations to Synthetic Alignment: Taking Stock

We began this article by examining RLHF's four fundamental limitations: scalability and resource constraints, human feedback quality issues, reward model vulnerabilities, and systemic governance challenges. We then explored the design space of synthetic alignment methods that have emerged in response. Now we can assess: which limitations have synthetic methods genuinely solved, which remain partially addressed, and what new challenges have emerged?

## From Limitations to Solutions: A Scorecard

Let's map each RLHF limitation to how synthetic alignment addresses it, evaluating the evidence for genuine progress versus merely shifted problems.

| RLHF Limitation | Status | Key Improvements | Remaining Challenges |
|----------------|--------|------------------|---------------------|
| **Scalability & Cost** | ✅ Solved | Automated preference generation eliminates human annotation bottleneck; methods generate tens of thousands of preference pairs without human cost | None—the economic constraint is decisively addressed |
| **Research Velocity** | ✅ Solved | Iteration cycles reduced from months to days; rapid algorithmic exploration enabled | None—temporal constraints eliminated |
| **Human Inconsistency** | ⚠️ Partially Solved | Perfectly consistent AI judge evaluations eliminate annotator disagreement and noise | Judge models introduce systematic biases (vs. random noise); correctness ≠ consistency |
| **Reward Hacking** | ⚠️ Partially Addressed | DPO methods eliminate explicit reward model exploitation; reduced staleness | Gaming shifts to judge scoring functions; self-judgment creates feedback loops; policy still optimizes proxies |
| **Distribution Shift** | ✅ Solved (at computational cost) | On-policy training maintains data-policy alignment; demonstrably more stable than offline methods | Requires constant data regeneration and judge interaction—high computational expense |
| **Value Alignment & Representation** | ⚠️ Improved Transparency | Constitutional principles make values explicit and modifiable; fine-grained control over optimization targets | Value authorship problem remains; cross-cultural representation unsolved; single value set scaled globally |
| **Post-Deployment Adaptation** | ⚠️ Easier to Iterate | Friction reduced for running new alignment iterations; no human coordination needed | Deployed models still frozen; no continual learning from user interactions; paradigm remains training-time vs. deployment-time |

**Detailed Analysis:**

**✅ Scalability and Cost: Solved**

The transformation here is unambiguous. Synthetic alignment fundamentally eliminates the human resource bottleneck that caps RLHF's scalability. Methods like ULTRAFEEDBACK (Cui et al., 2024) generate preference data using automated judges at scales that would be prohibitively expensive with human annotators. Yuan et al. (2025), Wu et al. (2024), and Dong et al. (2024) demonstrate iterative self-improvement over multiple rounds—3-4 iterations generating tens of thousands of preference pairs—without any human annotation beyond initial seed data. The economic constraint that Casper et al. (2023) identified as "a major bottleneck for RLHF" simply doesn't apply when AI systems generate their own training signal. From a pure cost perspective, synthetic alignment represents a decisive breakthrough.

**✅ Research Velocity: Solved**

The "research velocity problem" we identified—where RLHF strangles iteration cycles through temporal constraints—has been similarly resolved. Automated preference generation eliminates the weeks required to coordinate human annotators and collect statistically significant judgments. Researchers can now execute complete training iterations in days rather than months, enabling the rapid exploration of algorithmic design space that Bowman et al. (2022) identified as crucial for progress. The evidence is in the pace of methodological innovation itself: the field has moved from Constitutional AI (Bai et al., 2022) through multiple generations of refinement in just three years, a velocity unimaginable under RLHF's temporal constraints.

**⚠️ Human Inconsistency: Partially Solved**

Synthetic alignment addresses the noise problem convincingly. AI judges provide perfectly consistent evaluations—the same model with the same prompt will generate identical judgments, eliminating the annotator disagreement that Santurkar et al. (2023) and Stiennon et al. (2022) documented. This consistency dramatically improves the signal-to-noise ratio in preference data.

But consistency isn't the same as correctness. Judge models introduce their own biases, systematically rather than randomly. Where human annotators disagree about subjective preferences, AI judges impose a single perspective—whatever values and biases the judge model learned during its training. As Guo et al. (2024) demonstrate, judge prompting significantly affects output quality, revealing both controllability and fragility. Constitutional AI (Bai et al., 2022) makes this explicit by encoding specific principles, but even ostensibly neutral judge models carry implicit value judgments.

The supervision ceiling problem—human inability to evaluate superhuman capabilities—remains more speculative than solved. While AI judges could theoretically evaluate capabilities beyond human expertise, we lack strong evidence that current judge models (typically GPT-4 or Claude) possess genuinely superhuman evaluation capabilities. The judges are trained on human-generated data and inherit its limitations. The promise exists, but the empirical validation is pending.

**⚠️ Reward Hacking: Partially Addressed**

Direct preference optimization methods (DPO and variants) eliminate the explicit reward model that agents could exploit, addressing one vulnerability. By optimizing policy directly from preference pairs, methods like those deployed by Yuan et al. (2025), Shi et al. (2024), and Guo et al. (2024) avoid the reward model staleness and proxy gaming that Gao et al. (2022) identified as fundamental to RLHF.

But optimization still finds shortcuts. The policy can still learn to satisfy the judge's preferences in ways that don't align with the true underlying objective. The locus of gaming shifts from "exploit the reward model" to "exploit the judge model's scoring function," but the fundamental problem—optimizing a proxy rather than the true objective—persists. Cheng et al.'s (2025) tree-search refinement partially mitigates this by investing more compute to find genuinely better responses rather than adversarially optimized ones, but the vulnerability remains structural.

Moreover, self-judgment methods (Yuan et al., 2025; Wu et al., 2024) introduce new failure modes. When the policy judges its own outputs, biases can compound through feedback loops. Wu et al. (2024) observe judge scores clustering around 5 after iteration 4, suggesting response homogeneity—a potential sign of convergence to local optima or collapse to a narrow distribution that the judge favors.

**✅ Distribution Shift: Solved (at Computational Cost)**

The evidence here is among the strongest in the entire analysis. Guo et al. (2024) demonstrate conclusively that on-policy training—generating preference data from the evolving policy—maintains significantly better training stability than offline approaches. The distribution mismatch and reward model staleness that Askell et al. (2021) and Casper et al. (2023) identified as fundamental RLHF problems are directly solved by maintaining alignment between training data distribution and policy behavior.

Yuan et al. (2025), Wu et al. (2024), Cheng et al. (2025), Dong et al. (2024), and Yu et al. (2025) all adopt on-policy training and demonstrate stable multi-iteration improvement. The architectural solution is clear: keep the training data fresh by continuously regenerating it from the current policy.

The cost is computational rather than conceptual. On-policy training requires constant data regeneration and judge interaction, multiplying inference costs. But in an era where compute is increasingly abundant relative to high-quality human annotation, this trade-off strongly favors on-policy synthetic alignment over offline RLHF.

**⚠️ Value Alignment and Representation: Improved Transparency, Same Fundamental Challenge**

Constitutional AI (Bai et al., 2022) and related methods offer a significant improvement in transparency: the values being optimized for are explicitly stated as written principles rather than implicitly encoded in an opaque reward model. Kundu et al. (2023) demonstrate that specific principles outperform general ones, showing that we can exercise fine-grained control over which values are being optimized.

But whose values those principles encode remains unchanged. Where RLHF embedded "the values of the people who provide the feedback" (Casper et al., 2023) in reward models, Constitutional AI embeds the values of whoever writes the constitution. The representation problem—that annotator demographics aren't representative of global user populations—becomes a constitution authorship problem. The Indian Bias Evaluation Dataset example we discussed earlier would apply equally to a constitution authored without consideration of caste-based biases.

Synthetic alignment makes value choices more visible and modifiable, which is genuine progress. But it doesn't solve the fundamental challenge of value pluralism: how do we build systems that respect genuinely diverse preferences across cultures rather than scaling a single set of values to global deployment?

**⚠️ Post-Deployment Adaptation: Easier to Iterate, Fundamentally Still Static**

Synthetic alignment methods dramatically reduce the friction of updating alignment after deployment. Casper et al.'s (2023) observation that "RLHF is a one-shot process" applies with less force: you can run additional synthetic alignment iterations with new constitutional principles or updated judge models without coordinating human annotators. The temporal rigidity is reduced.

But the fundamental architecture remains static. Once deployed, models don't continue learning from real-world interactions. They don't adapt to evolving social norms or correct systematic errors based on deployment feedback. Iterative methods like Yuan et al. (2025) and Wu et al. (2024) demonstrate multi-round improvement, but these iterations happen during development, not deployment. The model shipped to production is still a frozen snapshot, unable to adapt as contexts change.

True post-deployment adaptation would require continuous learning from user interactions, incorporating feedback loops that current synthetic alignment methods don't support. The barriers are reduced—updating is easier—but the paradigm hasn't fundamentally changed from training-time alignment to deployment-time adaptation.

---

## From Assessment to Frontiers

The scorecard reveals both genuine progress and persistent challenges. Synthetic alignment decisively solves RLHF's scalability and research velocity problems, and on-policy training addresses distribution shift concerns. But judge model biases, value pluralism, and the 3-4 iteration ceiling remain thorny challenges that current methods haven't fully addressed.

Moreover, the six empirical insights from on-policy training's dominance to the importance of controlled comparisons—provide actionable guidance for practitioners. Yet they also reveal fundamental questions about sustainability: if self-improvement plateaus after a few iterations, are we building incrementally useful tools or transformative paradigm shifts?

The evidence suggests that synthetic alignment hasn't solved alignment, it's shifted and refined the challenge. Judge model biases replace human biases. Computational costs replace human costs. New failure modes emerge in judge-policy co-evolution dynamics. These aren't merely engineering challenges to be optimized away; they represent conceptual frontiers that will shape the field's next chapter.

**[Part 4](/blog/synthetic-alignment-future/)** explores these frontiers in depth, examining five critical research questions: from autonomous meta-alignment and safe post-deployment adaptation, to breaking the iteration ceiling, systematic judge bias auditing, and understanding co-evolution dynamics. These aren't incremental improvements—they're fundamental challenges that could reshape how we think about alignment itself.

---

## References

Askell, A., Bai, Y., Chen, A., et al., 2021. A General Language Assistant as a Laboratory for Alignment. https://doi.org/10.48550/arXiv.2112.00861

Bai, Y., Kadavath, S., Kundu, S., et al., 2022. Constitutional AI: Harmlessness from AI Feedback. https://doi.org/10.48550/arXiv.2212.08073

Bowman, S.R., Hyun, J., Perez, E., et al., 2022. Measuring Progress on Scalable Oversight for Large Language Models. https://doi.org/10.48550/arXiv.2211.03540

Casper, S., Davies, X., Shi, C., et al., 2023. Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. https://doi.org/10.48550/arXiv.2307.15217

Cheng, J., Liu, X., Wang, C., et al., 2025. SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models. https://doi.org/10.48550/arXiv.2412.11605

Cui, G., Yuan, L., Ding, N., et al., 2024. UltraFeedback: Boosting Language Models with Scaled AI Feedback. https://doi.org/10.48550/arXiv.2310.01377

Dong, Q., Dong, L., Zhang, X., Sui, Z., Wei, F., 2024. Self-Boosting Large Language Models with Synthetic Preference Data. https://doi.org/10.48550/arXiv.2410.06961

Gao, L., Schulman, J., Hilton, J., 2022. Scaling Laws for Reward Model Overoptimization. https://doi.org/10.48550/arXiv.2210.10760

Guo, S., Zhang, B., Liu, T., et al., 2024. Direct Language Model Alignment from Online AI Feedback. https://doi.org/10.48550/arXiv.2402.04792

Kim, S., Bae, S., Shin, J., et al., 2023. Aligning Large Language Models through Synthetic Feedback. https://doi.org/10.48550/arXiv.2305.13735

Kundu, S., Bai, Y., Kadavath, S., et al., 2023. Specific versus General Principles for Constitutional AI. https://doi.org/10.48550/arXiv.2310.13798

Lee, H., Phatale, S., Mansoor, H., et al., 2024. RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. https://doi.org/10.48550/arXiv.2309.00267

Ouyang, L., Wu, J., Jiang, X., et al., 2022. Training language models to follow instructions with human feedback. https://doi.org/10.48550/arXiv.2203.02155

Santurkar, S., Durmus, E., Ladhak, F., Lee, C., Liang, P., Hashimoto, T., 2023. Whose Opinions Do Language Models Reflect? https://doi.org/10.48550/arXiv.2303.17548

Shi, T., Chen, K., Zhao, J., 2024. Safer-Instruct: Aligning Language Models with Automated Preference Data. https://doi.org/10.48550/arXiv.2311.08685

Stiennon, N., Ouyang, L., Wu, J., et al., 2022. Learning to summarize from human feedback. https://doi.org/10.48550/arXiv.2009.01325

Wu, T., Yuan, W., Golovneva, O., et al., 2024. Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge. https://doi.org/10.48550/arXiv.2407.19594

Yu, T., Zhang, H., Li, Q., et al., 2025. RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness. https://doi.org/10.48550/arXiv.2405.17220

Yuan, W., Pang, R.Y., Cho, K., et al., 2025. Self-Rewarding Language Models. https://doi.org/10.48550/arXiv.2401.10020

Zweiger, A., Pari, J., Guo, H., et al., 2025. Self-Adapting Language Models. https://doi.org/10.48550/arXiv.2506.10943
